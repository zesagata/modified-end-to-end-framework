{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% importing library\n",
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Importing Library...\n",
      "Importing Library Success\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Importing Library...\")\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.utils.data as putils\n",
    "\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets,transforms\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import numpy\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "print(\"Importing Library Success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Creating Class...\nCreating Class Success\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Creating Class...\")\n",
    "\n",
    "class Codec():\n",
    "    def __init__(self, size, interpolate_size, mode):\n",
    "        self.size = size\n",
    "        self.interpolate_size = interpolate_size\n",
    "        self.mode = mode\n",
    "\n",
    "    def btc(self, image):\n",
    "        if image.shape[0] % self.size != 0:\n",
    "            n = ((self.size * int(image.shape[0] / self.size)) + self.size) - image.shape[0]\n",
    "            image = numpy.pad(array=image, pad_width=(0, n))\n",
    "\n",
    "        x = image.shape[0] / self.size\n",
    "        y = image.shape[1] / self.size\n",
    "        block_image = numpy.split(numpy.concatenate(numpy.split(image, y, axis=1)), x * y)\n",
    "        for i in range(len(block_image)):\n",
    "            mean = numpy.mean(numpy.mean(block_image[i], axis=1))\n",
    "            std = numpy.std(block_image[i])\n",
    "            m = self.size * self.size\n",
    "            q = numpy.sum(block_image[i] > mean)\n",
    "\n",
    "            a = mean - std * numpy.sqrt(q / (m - q))\n",
    "            b = mean + std * numpy.sqrt((m - q) / q)\n",
    "\n",
    "            block_image[i][block_image[i] > mean] = b\n",
    "            block_image[i][block_image[i] < mean] = a\n",
    "\n",
    "        block_image = numpy.concatenate(block_image)\n",
    "        block_image = numpy.split(block_image, x)\n",
    "\n",
    "        temp = numpy.concatenate(block_image, axis=1)\n",
    "\n",
    "        if image.shape[0] % self.size != 0:\n",
    "            temp = temp[1:-n + 1, 1:-n + 1]\n",
    "\n",
    "        return torch.Tensor(temp)\n",
    "\n",
    "    def BlockTruncationCoding(self, images_tensor):\n",
    "        results_tensor = images_tensor.new_empty(size=images_tensor.size())\n",
    "        for i in range(len(images_tensor)):\n",
    "            compact_image = images_tensor[i][0]\n",
    "            compressed_image = self.btc(compact_image)\n",
    "            results_tensor[i][0] = compressed_image\n",
    "        return results_tensor\n",
    "\n",
    "    def Interpolate(self, image):\n",
    "        return nn.functional.interpolate(input=image, scale_factor=self.interpolate_size, mode=self.mode,\n",
    "                                         align_corners=False)\n",
    "\n",
    "class RecCNN(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(RecCNN, self).__init__()\n",
    "        self.deconv1 = nn.Conv2d(channel, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64, affine=False)\n",
    "        self.deconv_n = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn_n = nn.BatchNorm2d(64, affine=False)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64, channel, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.deconv1(x))\n",
    "        for _ in range(18):\n",
    "            out = self.relu(self.bn_n(self.deconv_n(out)))\n",
    "        out = self.deconv3(out)\n",
    "        final = out.add(x)\n",
    "        return final, out\n",
    "class ComCNN(nn.Module):\n",
    "    def __init__(self, channel):\n",
    "        super(ComCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channel, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(64, affine=False)\n",
    "        self.conv3 = nn.Conv2d(64, channel, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.conv1(x))\n",
    "        out = self.relu(self.bn1(self.conv2(out)))\n",
    "        return self.conv3(out)\n",
    "\n",
    "\n",
    "def loss_function_l1(reconstructed_image,original_image):\n",
    "    return nn.MSELoss(size_average=False)(reconstructed_image,original_image)\n",
    "\n",
    "def loss_function_l2(residual_image,decoded_image,original_image):\n",
    "    return nn.MSELoss(size_average=False)(residual_image,original_image-decoded_image)\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    mse = numpy.mean( (img1 - img2) ** 2 )\n",
    "    if mse == 0:\n",
    "        return 100\n",
    "    PIXEL_MAX = 255.0\n",
    "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))\n",
    "\n",
    "import math\n",
    "irange = range\n",
    "\n",
    "\n",
    "def make_grid(tensor, nrow=8, padding=2,\n",
    "              normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    \"\"\"Make a grid of images.\n",
    "    Args:\n",
    "        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n",
    "            or a list of images all of the same size.\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid.\n",
    "            The Final grid size is (B / nrow, nrow). Default is 8.\n",
    "        padding (int, optional): amount of padding. Default is 2.\n",
    "        normalize (bool, optional): If True, shift the image to the range (0, 1),\n",
    "            by subtracting the minimum and dividing by the maximum pixel value.\n",
    "        range (tuple, optional): tuple (min, max) where min and max are numbers,\n",
    "            then these numbers are used to normalize the image. By default, min and max\n",
    "            are computed from the tensor.\n",
    "        scale_each (bool, optional): If True, scale each image in the batch of\n",
    "            images separately rather than the (min, max) over all images.\n",
    "        pad_value (float, optional): Value for the padded pixels.\n",
    "    Example:\n",
    "        See this notebook `here <https://gist.github.com/anonymous/bf16430f7750c023141c562f3e9f2a91>`_\n",
    "    \"\"\"\n",
    "    if not (torch.is_tensor(tensor) or\n",
    "            (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n",
    "        raise TypeError('tensor or list of tensors expected, got {}'.format(type(tensor)))\n",
    "\n",
    "    # if list of tensors, convert to a 4D mini-batch Tensor\n",
    "    if isinstance(tensor, list):\n",
    "        tensor = torch.stack(tensor, dim=0)\n",
    "\n",
    "    if tensor.dim() == 2:  # single image H x W\n",
    "        tensor = tensor.view(1, tensor.size(0), tensor.size(1))\n",
    "    if tensor.dim() == 3:  # single image\n",
    "        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n",
    "            tensor = torch.cat((tensor, tensor, tensor), 0)\n",
    "        tensor = tensor.view(1, tensor.size(0), tensor.size(1), tensor.size(2))\n",
    "\n",
    "    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n",
    "        tensor = torch.cat((tensor, tensor, tensor), 1)\n",
    "\n",
    "    if normalize is True:\n",
    "        tensor = tensor.clone()  # avoid modifying tensor in-place\n",
    "        if range is not None:\n",
    "            assert isinstance(range, tuple), \\\n",
    "                \"range has to be a tuple (min, max) if specified. min and max are numbers\"\n",
    "\n",
    "        def norm_ip(img, min, max):\n",
    "            img.clamp_(min=min, max=max)\n",
    "            img.add_(-min).div_(max - min + 1e-5)\n",
    "\n",
    "        def norm_range(t, range):\n",
    "            if range is not None:\n",
    "                norm_ip(t, range[0], range[1])\n",
    "            else:\n",
    "                norm_ip(t, float(t.min()), float(t.max()))\n",
    "\n",
    "        if scale_each is True:\n",
    "            for t in tensor:  # loop over mini-batch dimension\n",
    "                norm_range(t, range)\n",
    "        else:\n",
    "            norm_range(tensor, range)\n",
    "\n",
    "    if tensor.size(0) == 1:\n",
    "        return tensor.squeeze()\n",
    "\n",
    "    # make the mini-batch of images into a grid\n",
    "    nmaps = tensor.size(0)\n",
    "    xmaps = min(nrow, nmaps)\n",
    "    ymaps = int(math.ceil(float(nmaps) / xmaps))\n",
    "    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n",
    "    grid = tensor.new(3, height * ymaps + padding, width * xmaps + padding).fill_(pad_value)\n",
    "    k = 0\n",
    "    for y in irange(ymaps):\n",
    "        for x in irange(xmaps):\n",
    "            if k >= nmaps:\n",
    "                break\n",
    "            grid.narrow(1, y * height + padding, height - padding)\\\n",
    "                .narrow(2, x * width + padding, width - padding)\\\n",
    "                .copy_(tensor[k])\n",
    "            k = k + 1\n",
    "    return grid\n",
    "\n",
    "\n",
    "def save_image(tensor, filename, nrow=8, padding=2,\n",
    "               normalize=False, range=None, scale_each=False, pad_value=0):\n",
    "    \"\"\"Save a given Tensor into an image file.\n",
    "    Args:\n",
    "        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n",
    "            saves the tensor as a grid of images by calling ``make_grid``.\n",
    "        **kwargs: Other arguments are documented in ``make_grid``.\n",
    "    \"\"\"\n",
    "    from PIL import Image\n",
    "    grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n",
    "                     normalize=normalize, range=range, scale_each=scale_each)\n",
    "    ndarr = grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(filename)\n",
    "\n",
    "\n",
    "print(\"Creating Class Success\")\n",
    " "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Create Class\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Load Image Dataset\nLoad Image Dataset Success\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Load Image Dataset\")\n",
    "train_image_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(size=(96,96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((105.97411112882654,),(63.11390135470555,))\n",
    "])\n",
    "\n",
    "test_image_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize(size=(96,96)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((85.35398198341836,),(65.0054783896806,))\n",
    "])\n",
    "\n",
    "\n",
    "train_path = '../dataset/train/'\n",
    "test_path = '../dataset/test/'\n",
    "train_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=train_path,\n",
    "    transform=train_image_transform\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\n",
    "    root=test_path,\n",
    "    transform=test_image_transform\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=16,\n",
    "                                          shuffle=True,num_workers=2)\n",
    "\n",
    "print(\"Load Image Dataset Success\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Prepared Dataset\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Initialize Model....\n",
      "Cuda is available, using gpu instead\nInitialize Model Success\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Initialize Model....\")\n",
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "codec = Codec(4,2,'bicubic')\n",
    "\n",
    "if CUDA:\n",
    "    comCNN = ComCNN(1).cuda()\n",
    "    recCNN = RecCNN(1).cuda()\n",
    "    print(\"Cuda is available, using gpu instead\")\n",
    "else:\n",
    "    comCNN = ComCNN(1)\n",
    "    recCNN = RecCNN(1)\n",
    "    print(\"Cuda is not available, using cpu instead\")\n",
    "\n",
    "comCNNOptimizer = optim.Adam(comCNN.parameters(),lr=1e-3)\n",
    "\n",
    "recCNNOptimizer = optim.Adam(recCNN.parameters(),lr=1e-3)\n",
    "\n",
    "print(\"Initialize Model Success\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Create a model\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Begin Training....\n",
      "====> Epoch: 0\nAverage loss 1: 29144.3854\nAverage Loss 2: 32093.8403\nRunning Time : 27.49280047416687\n",
      "====> Epoch: 1\nAverage loss 1: 1104.9644\nAverage Loss 2: 1047.9704\nRunning Time : 25.814172744750977\n",
      "====> Epoch: 2\nAverage loss 1: 398.9957\nAverage Loss 2: 298.6641\nRunning Time : 26.167897939682007\n",
      "====> Epoch: 3\nAverage loss 1: 297.5458\nAverage Loss 2: 175.2251\nRunning Time : 25.900792121887207\n",
      "====> Epoch: 4\nAverage loss 1: 263.8068\nAverage Loss 2: 117.6076\nRunning Time : 25.866865873336792\n",
      "====> Epoch: 5\nAverage loss 1: 227.4037\nAverage Loss 2: 87.1904\nRunning Time : 25.9606511592865\n",
      "====> Epoch: 6\nAverage loss 1: 212.1080\nAverage Loss 2: 71.1275\nRunning Time : 25.992525577545166\n",
      "====> Epoch: 7\nAverage loss 1: 198.7198\nAverage Loss 2: 63.4024\nRunning Time : 25.78623652458191\n",
      "====> Epoch: 8\nAverage loss 1: 181.1405\nAverage Loss 2: 55.1394\nRunning Time : 25.75716257095337\n",
      "====> Epoch: 9\nAverage loss 1: 167.0403\nAverage Loss 2: 49.1511\nRunning Time : 25.792076587677002\n",
      "====> Epoch: 10\nAverage loss 1: 159.4459\nAverage Loss 2: 48.0656\nRunning Time : 25.93074369430542\n",
      "====> Epoch: 11\nAverage loss 1: 145.5672\nAverage Loss 2: 46.6390\nRunning Time : 25.889968156814575\n",
      "====> Epoch: 12\nAverage loss 1: 131.3364\nAverage Loss 2: 36.7162\nRunning Time : 25.862861156463623\n",
      "====> Epoch: 13\nAverage loss 1: 125.3039\nAverage Loss 2: 33.7458\nRunning Time : 26.248987436294556\n",
      "====> Epoch: 14\nAverage loss 1: 121.3409\nAverage Loss 2: 46.2506\nRunning Time : 25.884816646575928\n",
      "====> Epoch: 15\nAverage loss 1: 109.9574\nAverage Loss 2: 54.3532\nRunning Time : 25.958603143692017\n",
      "====> Epoch: 16\nAverage loss 1: 92.0845\nAverage Loss 2: 48.9637\nRunning Time : 28.654611825942993\n",
      "====> Epoch: 17\nAverage loss 1: 63.0632\nAverage Loss 2: 38.4927\nRunning Time : 26.19793438911438\n",
      "====> Epoch: 18\nAverage loss 1: 63.4473\nAverage Loss 2: 39.6542\nRunning Time : 28.128143072128296\n",
      "====> Epoch: 19\nAverage loss 1: 47.7580\nAverage Loss 2: 27.0682\nRunning Time : 26.196027517318726\n",
      "====> Epoch: 20\nAverage loss 1: 44.1596\nAverage Loss 2: 21.9065\nRunning Time : 25.929687976837158\n",
      "====> Epoch: 21\nAverage loss 1: 42.1227\nAverage Loss 2: 19.0972\nRunning Time : 26.04051375389099\n",
      "====> Epoch: 22\nAverage loss 1: 41.4621\nAverage Loss 2: 17.1266\nRunning Time : 25.865010976791382\n",
      "====> Epoch: 23\nAverage loss 1: 41.0082\nAverage Loss 2: 15.5003\nRunning Time : 25.89479112625122\n",
      "====> Epoch: 24\nAverage loss 1: 40.1903\nAverage Loss 2: 14.2398\nRunning Time : 25.961599826812744\n",
      "====> Epoch: 25\nAverage loss 1: 40.1680\nAverage Loss 2: 15.0535\nRunning Time : 25.955674409866333\n",
      "====> Epoch: 26\nAverage loss 1: 33.3379\nAverage Loss 2: 18.9176\nRunning Time : 26.160595655441284\n",
      "====> Epoch: 27\nAverage loss 1: 27.5070\nAverage Loss 2: 27.8719\nRunning Time : 25.902525186538696\n",
      "====> Epoch: 28\nAverage loss 1: 22.7221\nAverage Loss 2: 17.6922\nRunning Time : 25.875901222229004\n",
      "====> Epoch: 29\nAverage loss 1: 21.1556\nAverage Loss 2: 15.1311\nRunning Time : 25.80415678024292\n",
      "====> Epoch: 30\nAverage loss 1: 19.4291\nAverage Loss 2: 13.3146\nRunning Time : 26.244811534881592\n",
      "====> Epoch: 31\nAverage loss 1: 19.4093\nAverage Loss 2: 13.4863\nRunning Time : 25.820854663848877\n",
      "====> Epoch: 32\nAverage loss 1: 17.3027\nAverage Loss 2: 10.6656\nRunning Time : 25.824994802474976\n",
      "====> Epoch: 33\nAverage loss 1: 20.4394\nAverage Loss 2: 14.2025\nRunning Time : 25.81100106239319\n",
      "====> Epoch: 34\nAverage loss 1: 29.8132\nAverage Loss 2: 22.6772\nRunning Time : 25.710312843322754\n",
      "====> Epoch: 35\nAverage loss 1: 19.0577\nAverage Loss 2: 13.4495\nRunning Time : 25.859822273254395\n",
      "====> Epoch: 36\nAverage loss 1: 16.2724\nAverage Loss 2: 9.5967\nRunning Time : 26.039512395858765\n",
      "====> Epoch: 37\nAverage loss 1: 14.5919\nAverage Loss 2: 7.8833\nRunning Time : 25.924736261367798\n",
      "====> Epoch: 38\nAverage loss 1: 13.9844\nAverage Loss 2: 7.1274\nRunning Time : 25.84889554977417\n",
      "====> Epoch: 39\nAverage loss 1: 13.5247\nAverage Loss 2: 6.5489\nRunning Time : 25.878910541534424\n",
      "====> Epoch: 40\nAverage loss 1: 13.5571\nAverage Loss 2: 6.1614\nRunning Time : 25.805066108703613\n",
      "====> Epoch: 41\nAverage loss 1: 13.4913\nAverage Loss 2: 5.9457\nRunning Time : 25.770084142684937\n",
      "====> Epoch: 42\nAverage loss 1: 13.4784\nAverage Loss 2: 5.7331\nRunning Time : 25.76024866104126\n",
      "====> Epoch: 43\nAverage loss 1: 13.6070\nAverage Loss 2: 6.2364\nRunning Time : 25.899759769439697\n",
      "====> Epoch: 44\nAverage loss 1: 13.5163\nAverage Loss 2: 5.5016\nRunning Time : 25.851929426193237\n",
      "====> Epoch: 45\nAverage loss 1: 13.3117\nAverage Loss 2: 6.4673\nRunning Time : 25.804020643234253\n",
      "====> Epoch: 46\nAverage loss 1: 13.1735\nAverage Loss 2: 8.4677\nRunning Time : 25.726258277893066\n",
      "====> Epoch: 47\nAverage loss 1: 14.0741\nAverage Loss 2: 9.6330\nRunning Time : 25.79607582092285\n",
      "====> Epoch: 48\nAverage loss 1: 12.6689\nAverage Loss 2: 6.9978\nRunning Time : 26.023463487625122\n",
      "====> Epoch: 49\nAverage loss 1: 11.3109\nAverage Loss 2: 4.8301\nRunning Time : 25.792059421539307\n\n========>Training Complete....\nTotal Running Time : 1302.0828754901886\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "d:\\pycharmprojects\\end-to-end-compression-framework\\venv\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n  warnings.warn(warning.format(ret))\n",
      "d:\\pycharmprojects\\end-to-end-compression-framework\\venv\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in long_scalars\nd:\\pycharmprojects\\end-to-end-compression-framework\\venv\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in multiply\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import time\n",
    "print(\"Begin Training....\")\n",
    "epochs = 50\n",
    "comCNN.train()\n",
    "recCNN.train()\n",
    "graph_l1 = np.empty(epochs)\n",
    "graph_l2 = np.empty(epochs)\n",
    "global_start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    train_loss_1 = 0\n",
    "    train_loss_2 = 0\n",
    "    for batch_idx,(data,_) in enumerate(train_loader):\n",
    "        \n",
    "        data = Variable(data)\n",
    "        \n",
    "        compact_image = comCNN(data.cuda())\n",
    "        compact_image_temp = compact_image.cpu().detach()\n",
    "        compressed_image_btc = codec.BlockTruncationCoding(compact_image_temp)\n",
    "        upscaled_image_btc = codec.Interpolate(compressed_image_btc)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # update beta with eq 5\n",
    "    \n",
    "        recCNNOptimizer.zero_grad()\n",
    "        reconstructed_image,residual = recCNN(upscaled_image_btc.cuda())\n",
    "        loss2 = loss_function_l2(\n",
    "            residual_image=residual,\n",
    "            decoded_image=upscaled_image_btc.cuda(),\n",
    "            original_image=data.cuda()\n",
    "        )\n",
    "        loss2.backward()\n",
    "        \n",
    "        train_loss_2 += loss2.item()\n",
    "        recCNNOptimizer.step()\n",
    "    \n",
    "    \n",
    "        comCNNOptimizer.zero_grad()\n",
    "        upscaled_image_nonbtc = codec.Interpolate(compact_image)\n",
    "        reconstructed_without_btc,_ = recCNN(upscaled_image_nonbtc.cuda())\n",
    "        loss1 = loss_function_l1(\n",
    "            reconstructed_image=reconstructed_without_btc.cuda(),\n",
    "            original_image=data.cuda()\n",
    "        )\n",
    "        loss1.backward()\n",
    "        train_loss_1 += loss1.item()\n",
    "        comCNNOptimizer.step()\n",
    "        # print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} - {:.6f}'.format(\n",
    "        #         epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "        #         100. * batch_idx / len(train_loader),\n",
    "        #         loss1.item() / len(data) , loss2.item()/len(data)))\n",
    "    end = time.time()\n",
    "    graph_l1[epoch] = train_loss_1/len(train_loader.dataset)\n",
    "    graph_l2[epoch] = train_loss_2/len(train_loader.dataset)\n",
    "    print('====> Epoch: {}\\nAverage loss 1: {:.4f}\\nAverage Loss 2: {:.4f}\\nRunning Time : {}'.format(\n",
    "          epoch, train_loss_1/len(train_loader.dataset),train_loss_2/len(train_loader.dataset),(end-start)))\n",
    "\n",
    "global_end = time.time()\n",
    "print(\"\\n========>Training Complete....\")\n",
    "print('Total Running Time : {}'.format((global_end-global_start)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Train\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Saving Model...\n",
      "Model saved\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "d:\\pycharmprojects\\end-to-end-compression-framework\\venv\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ComCNN. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "d:\\pycharmprojects\\end-to-end-compression-framework\\venv\\lib\\site-packages\\torch\\serialization.py:256: UserWarning: Couldn't retrieve source code for container of type RecCNN. It won't be checked for correctness upon loading.\n  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Saving Model...\")\n",
    "torch.save(comCNN,'../model/ComCNN-with-btc[normalize]96x96.pt')\n",
    "torch.save(recCNN,'../model/RecCNN-with-btc[normalize]96x96.pt')\n",
    "print(\"Model saved\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Saving Model\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.plot(graph_l1)\n",
    "plt.title(\"MSE L1\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.plot(graph_l2)\n",
    "plt.title('MSE L2')\n",
    "plt.grid(True)\n",
    "plt.subplots_adjust(top=0.92, bottom=0.08, left=0.10, right=0.95, hspace=0.25,\n",
    "                    wspace=0.35)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% create graph\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Load trained model...\n",
      "Load success\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "print(\"Load trained model...\")\n",
    "trainedComCNN = torch.load('../model/ComCNN-with-btc[normalize]96x96.pt')\n",
    "trainedRecCNN = torch.load('../model/RecCNN-with-btc[normalize]96x96.pt')\n",
    "codec = Codec(4,2,'bicubic')\n",
    "print(\"Load success\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Load Model\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Begin Testing....\n",
      "\n========>Teesting Complete....\nTotal Running Time : 29.20913076400757\nLoss Function ComCNN Average : 3295.821875\nLoss Function RecCNN Average : 3264.137890625\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "d:\\pycharmprojects\\end-to-end-compression-framework\\venv\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: divide by zero encountered in long_scalars\nd:\\pycharmprojects\\end-to-end-compression-framework\\venv\\lib\\site-packages\\ipykernel_launcher.py:24: RuntimeWarning: invalid value encountered in multiply\n",
      "d:\\pycharmprojects\\end-to-end-compression-framework\\venv\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n  warnings.warn(warning.format(ret))\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import time\n",
    "trainedComCNN.eval()\n",
    "trainedRecCNN.eval()\n",
    "print(\"Begin Testing....\")\n",
    "\n",
    "graph_l1_test = np.empty(len(test_loader.dataset))\n",
    "graph_l2_test = np.empty(len(test_loader.dataset))\n",
    "global_start = time.time()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for batch_idx,(data,_) in enumerate(test_loader):\n",
    "    data = Variable(data)\n",
    "    compact_image = trainedComCNN(data.cuda())\n",
    "    compact_image_temp = compact_image.cpu().detach()\n",
    "    compressed_image_btc = codec.BlockTruncationCoding(compact_image_temp)\n",
    "    upscaled_image_btc = codec.Interpolate(compressed_image_btc)\n",
    "    \n",
    "    \n",
    "    reconstructed_image,residual = trainedRecCNN(upscaled_image_btc.cuda())\n",
    "    loss2 = loss_function_l2(\n",
    "            residual_image=residual,\n",
    "            decoded_image=upscaled_image_btc.cuda(),\n",
    "            original_image=data.cuda()\n",
    "        )\n",
    "    \n",
    "    upscaled_image_nonbtc = codec.Interpolate(compact_image)\n",
    "    reconstructed_without_btc,_ = trainedRecCNN(upscaled_image_nonbtc.cuda())\n",
    "    loss1 = loss_function_l1(\n",
    "            reconstructed_image=reconstructed_without_btc.cuda(),\n",
    "            original_image=data.cuda()\n",
    "        )\n",
    "    \n",
    "    graph_l1_test[i] = loss1.item()\n",
    "    \n",
    "    graph_l2_test[i] = loss2.item()\n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "global_end = time.time()\n",
    "print(\"\\n========>Teesting Complete....\")\n",
    "print('Total Running Time : {}'.format((global_end-global_start)))\n",
    "print('Loss Function ComCNN Average : {}'.format(np.mean(graph_l1_test)))\n",
    "print('Loss Function RecCNN Average : {}'.format(np.mean(graph_l2_test)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% Test Model\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Test Set 0 -> psnr : 52.59992089774271\n",
      "Test Set 1 -> psnr : 52.64837229021972\nTest Set 2 -> psnr : 52.657510966815124\nTest Set 3 -> psnr : 52.64137318166708\nTest Set 4 -> psnr : 52.631422595082036\nTest Set 5 -> psnr : 52.658505006027426\nTest Set 6 -> psnr : 52.66740348842458\nTest Set 7 -> psnr : 52.61956742941944\nTest Set 8 -> psnr : 52.63147111838835\nTest Set 9 -> psnr : 52.630315462438695\nAverage psnr : 52.63858624362251\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "psnr_total = 0\n",
    "for i in range(len(data)):\n",
    "    img1 = reconstructed_image[i][0].cpu().detach().numpy()\n",
    "    img2 = data[i][0].cpu().detach().numpy()\n",
    "    psnr_temp = psnr(img1,img2)\n",
    "    print(\"Test Set {} -> psnr : {}\".format(i,psnr_temp))\n",
    "    psnr_total += psnr_temp\n",
    "    save_image(data[i],'../result/end-to-end-with-btc[normalize]96x96/ori-{}.png'.format(i))\n",
    "    save_image(reconstructed_image[i],'../result/end-to-end-with-btc[normalize]96x96/recon-{}.png'.format(i))\n",
    "\n",
    "save_image(data[:],'../result/end-to-end-with-btc[normalize]96x96/original.png')    \n",
    "save_image(reconstructed_image[:],'../result/end-to-end-with-btc[normalize]96x96/recon.png')\n",
    "print(\"Average psnr : {}\".format(psnr_total/10.0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%  Show Image\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}